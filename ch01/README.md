## 벡터의 내적

- 두 벡터가 얼마나 같은 방향을 향하고 있는가

## 계층 클래스 (layer class)

- Affine 계층, Sigmoid 계층 등 모듈화

### 규칙

1. 모든 계층은 `forward()`와 `backward()` 메소드를 갖는다
2. 모든 계층은 인스턴스 변수인 `params`와 `grads`를 갖는다.
   - params : 가중치, 편향 등 매개변수
   - grads : 기울기 보관

---

## 신경망 학습 (Backward)

### 기울기

- 벡터(또는 행렬)의 각 원소에 대한 편미분을 정리한 것
- 이 때, 행렬과 그 기울기의 형상이 같음 => 매개변수 갱신과 연쇄 법칙을 쉽게 구현 가능

### 연쇄 법칙

- 합성함수에 대한 미분의 법칙
- 각 함수의 미분만 알면 그 값들을 곱해서 전체의 미분을 계산할 수 있다.
- 우리가 원하는 것은 각 **매개변수에 대한 Loss의 기울기**
  - 연쇄법칙을 이용하는 `오차역전파법(back-propagation)` 사용

### 파이썬 문법 참고

- a = b 와 a[...] = b 차이점
  - a = b: 메모리 위치까지 같아짐, a 수정하면 b도 바뀜
  - a[...] = b: 값이 복사됨, a 수정하면 b는 안바뀜 (메모리 주소 고정)
  - 그래서 backward에서 기울기 저장할 때 사용 (학습할 때마다 기울기 바뀌면 다른 매개변수 안 바뀌게 하려고)

### Softmax-with-Loss 계층

- 이 계층의 역전파는 y-t 이다. (출력결과와 정답레이블의 차이)
  - **이 차이(역전파)를 앞 계층에 전달해 주는 것은 신경망 학습에서 아주 중요함!!**

### 가중치 갱신

1. 미니배치
2. 기울기 계산 (back-propagation)
3. 매개변수 갱신
4. 1\~3 반복

#### Gradient Descent

- back-propagation에서 구한 기울기는 현재의 가중치 매개변수에서 손실을 가장 크게 하는 방향을 가리킴
- 따라서 매개변수를 그 기울기와 반대 방향으로 갱신하면 손실을 줄일 수 있음
- `SGD`, `모멘텀`, `AdaGrad`, `Adam` 등

## 학습

### decision boundary

- 학습 후 신경망이 어떻게 클래스를 구별했는지 시각화

## 딥러닝 가속화

- 비트 정밀도
  - 데이터 비트 수가 작을 수록 속도에 유리 (성능에는 크게 영향 X)
- GPU
  - `cupy`: numpy와 공통된 API 제공하는 파이썬 라이브러리, 엔디비아 GPU에서만 동작
